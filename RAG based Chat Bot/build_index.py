# -*- coding: utf-8 -*-
"""build_index.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YiYZfsT72r3x_voKTc0abpvPPS9NJ1_u

**It reads Conversation.csv, creates sentence embeddings, builds a FAISS index (cosine similarity), and saves everything to rag.pkl**
"""

!pip install faiss-cpu
!pip install faiss-cpu

import os
import json
import pickle
import argparse
from pathlib import Path

import numpy as np
import pandas as pd
import faiss
from sentence_transformers import SentenceTransformer
from sklearn.preprocessing import normalize

def load_data(csv_path):
    df = pd.read_csv(csv_path)
    # Expect columns named 'question' and 'answer' (case-insensitive)
    cols = {c.lower(): c for c in df.columns}
    if 'question' not in cols or 'answer' not in cols:
        raise ValueError("CSV must contain 'question' and 'answer' columns (case-insensitive).")
    questions = df[cols['question']].astype(str).tolist()
    answers = df[cols['answer']].astype(str).tolist()
    return questions, answers, df

def build_index(questions, model_name="sentence-transformers/all-MiniLM-L6-v2", normalize_vectors=True):
    print(f"Loading embedding model: {model_name}")
    model = SentenceTransformer(model_name)
    print("Encoding questions ...")
    embeddings = model.encode(questions, convert_to_numpy=True, show_progress_bar=True)
    if normalize_vectors:
        embeddings = normalize(embeddings, axis=1)  # make unit length for cosine via dot product
    dimension = embeddings.shape[1]

    # Use IndexFlatIP and add normalized embeddings so dot product == cosine similarity
    index = faiss.IndexFlatIP(dimension)
    print(f"Adding {len(embeddings)} vectors to FAISS index (dim={dimension}) ...")
    index.add(embeddings.astype('float32'))

    return {
        "model_name": model_name,
        "questions": questions,
        "answers": answers,
        "index": index,
        "embeddings": embeddings  # stored optionally; big but convenient for debugging
    }

def save_rag(output_path, payload):
    with open(output_path, "wb") as f:
        pickle.dump(payload, f)
    print(f"Saved RAG payload to {output_path}")

if __name__ == "__main__":
  import sys
  sys.argv = sys.argv[:1]
  parser = argparse.ArgumentParser()
  parser.add_argument("--csv", type=str, default="Conversation.csv", help="Path to Conversation.csv")
  parser.add_argument("--out", type=str, default="rag.pkl", help="Output file (rag.pkl)")
  parser.add_argument("--model", type=str, default="sentence-transformers/all-MiniLM-L6-v2",
                      help="Sentence-Transformers model name")
  args = parser.parse_args()

  if not Path(args.csv).exists():
      raise FileNotFoundError(f"{args.csv} not found. Put Conversation.csv in same folder or pass --csv path.")

  questions, answers, df = load_data(args.csv)
  payload = build_index(questions, model_name=args.model)
  # We don't need to keep embeddings in memory at inference, but it's fine to save them.
  # Save original dataframe too for reference
  payload["df"] = df
  save_rag(args.out, payload)

pd.read_csv("Conversation.csv").head()

