# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jicoSgI6WON4uLcuXA1s7KrKeJbPBR91
"""

!pip install faiss-cpu

import pickle

"""Loads Python’s pickle module, which lets the program read and write Python objects to disk (here we use it to load rag.pkl that contains the index, questions, and answers)."""

import gradio as gr

"""Imports Gradio, a library that makes building web UIs (like chat apps) very easy. We use it to create the chat interface."""

from sentence_transformers import SentenceTransformer

"""Imports the Sentence-Transformers model wrapper. This gives us a pre-trained model that converts text into numeric vectors (embeddings) that capture semantic meaning."""

import random
import numpy as np
from sklearn.preprocessing import normalize

"""
loads the RAG data from a file. Default file name is rag.pkl

Opens the rag.pkl file in binary read mode
and uses pickle.load to restore the Python object we saved earlier
(it contains index, questions, answers, and model name).

Loads the Sentence-Transformer model specified inside data.
If data doesn’t contain a model name, it falls back to "sentence-transformers/all-MiniLM-L6-v2".
This model will be used to convert user queries into embeddings.

Returns a dictionary with:
  "model": the loaded embedding model,
  "index": the FAISS index (vector store),
  "questions": list of original questions from the CSV,
  "answers": corresponding list of answers.
"""
def load_rag(path="rag.pkl"):
    with open(path, "rb") as f:
        data = pickle.load(f)
    model = SentenceTransformer(data.get("model_name", "sentence-transformers/all-MiniLM-L6-v2"))
    # Normalize function: we'll normalize query to match index built with normalized vectors
    return {
        "model": model,
        "index": data["index"],
        "questions": data["questions"],
        "answers": data["answers"]
    }

# Calls load_rag immediately when the script starts and loads rag.pkl into memory. This makes RAG available for requests.
RAG = load_rag("rag.pkl")
MODEL = RAG["model"] # the sentence-transformer model object.
INDEX = RAG["index"] # the FAISS index we query to find similar vectors.
# QUESTIONS / ANSWERS : lists used to show retrieved Q/A.
QUESTIONS = RAG["questions"]
ANSWERS = RAG["answers"]

"""
Defines the function retrieve_answer that takes a user query string
and searches the index for the top k (default 5) similar questions.
"""
def retrieve_answer(query, top_k=5):
    """
    Quick guard clause: if the user input is empty or only whitespace, return a prompt asking the user to type something.
    """
    if not query or query.strip() == "":
        return {"answer": "Please type a question.", "meta": None}
    """
    Converts the input query into a numeric vector (embedding) using the pre-loaded MODEL.
    convert_to_numpy=True returns a NumPy array
    """
    qvec = MODEL.encode([query], convert_to_numpy=True)
    """
    Normalizes the query vector to unit length (so cosine similarity can be computed as dot product)
    and ensures the dtype is float32 (FAISS expects float32 vectors).
    """
    qvec = normalize(qvec, axis=1).astype('float32')  # must be normalized to match training normalization

    # search
    k = min(top_k, INDEX.ntotal)
    """
    k is the number of neighbors to request from FAISS.
    INDEX.ntotal is how many vectors are in the index;
    min() prevents asking for more neighbors than exist.
    """
    distances, indices = INDEX.search(qvec, k)  # since IndexFlatIP and normalized -> cosine scores in distances
    """
    Searches the FAISS index and returns:
    distances: similarity scores (for IndexFlatIP with normalized vectors these act like cosine similarity),
    indices: the index positions of the top k vectors (these point to entries in QUESTIONS / ANSWERS).
    """
    scores = distances[0].tolist()
    idxs = indices[0].tolist()
    """
    Extracts the first (and only) row from distances and indices (because we encoded a single query)
    and converts them into regular Python lists.
    """

    # Convert IP scores to cosine in [-1,1]; they already are in that range for normalized vectors
    results = []
    for score, idx in zip(scores, idxs):
        if idx < 0:
            continue
        results.append({"score": float(score), "question": QUESTIONS[idx], "answer": ANSWERS[idx], "idx": int(idx)})
    """
    Builds a results list of dictionaries for the top matches. Each dictionary includes:
      score: similarity score (float),
      question: the matched question from the dataset,
      answer: the matched answer,
      idx: the integer index of the match.
    It skips negative indices (FAISS might use -1 for invalids in some configs).
    """

    # Choose best answer with confidence threshold
    best = results[0] if results else None
    # Confidence threshold (tweakable)
    threshold = 0.45
    if best and best["score"] >= threshold:
        return {"answer": best["answer"], "meta": results}
    # fallback: if top score is not confident, still show the top answer but with a fallback message
    if best:
        fallback_text = (
            f"I found something that might help (confidence {best['score']:.2f}):\n\n"
            f"{best['answer']}\n\nIf that doesn't answer your question, try rephrasing it."
        )
        return {"answer": fallback_text, "meta": results}
    return {"answer": "Sorry — I couldn't find a relevant answer. Try rephrasing.", "meta": results}

"""
This function is called by Gradio whenever the user submits a message.
history is the current chat conversation (list of pairs), passed back and forth so the chat UI can show context.
"""
def chat_fn(query, history):
    res = retrieve_answer(query, top_k=3) # Calls retrieve_answer to find the best reply.
    bot_reply = res["answer"] # Extracts the string bot reply.
    # Append to chat history
    history = history or []
    # append two tuples: the user message and the bot reply. Each tuple is (speaker, message).
    history.append(("You", query))
    history.append(("Bot", bot_reply))
    return history, history

with gr.Blocks(title="RAG Chatbot (Conversation dataset)", css="footer {display:none}") as demo:
  """
  Creates a Gradio Blocks layout with a page title and hides the footer.
  Everything inside this block becomes part of the UI.
  """
  gr.Markdown("## RAG Chatbot — retrieves best reply from your Conversation.csv knowledge base")
  # Displays a heading at the top of the interface.
  chat = gr.Chatbot(elem_id="chatbot", label="Conversation")
  # Creates a chat-style message window where user and bot messages appear.
  msg = gr.Textbox(show_label=False, placeholder="Ask something...", lines=1)
  # A single-line text input where the user types their question.
  with gr.Row():
      submit = gr.Button("Send")
      clear = gr.Button("Clear")
  """
  Creates two buttons in a horizontal row:
    Send → submits the user message
    Clear → clears the chat history
  """
  submit.click(fn=chat_fn, inputs=[msg, chat], outputs=[chat, chat])
  clear.click(lambda: None, [], chat, queue=False)
  """
  Links the Send button to the chatbot logic (chat_fn).
  - It passes:
    - the message text
    - the current chat history

  Links the Clear button with chatbot login (chat_fn)
  - Clears the chat by returning an empty history.

  And returns an updated chat window.
  """
  gr.Examples(examples=[
      ["hi, how are you doing?"],
      ["How do I reset my password?"],
      ["how smart are you?"]
  ], inputs=msg)
  # Adds clickable example questions that auto-fill into the input box for quick testing.
  gr.Markdown("You can update your Conversation.csv and rebuild rag.pkl with `build_index.py` to change the knowledge base.")
  # Shows a note reminding how to update the knowledge base.
if __name__ == "__main__":
  demo.launch(server_name="0.0.0.0", server_port=7860)

